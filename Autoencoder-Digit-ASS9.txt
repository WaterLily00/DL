9 Implement simple autoencoder to reconstruct MNIST digits. Add sparsity
constraint on the encoded representations .,

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras import regularizers

# ------------------------------------------------------------
# üîπ WHAT IS AN AUTOENCODER?

# An Autoencoder is a type of neural network used for unsupervised learning.
# It learns to encode (compress) the input data into a smaller representation,
# called a "latent space", and then decode it back to reconstruct the original input.
#
# Architecture:
#   Input ‚Üí Encoder ‚Üí Latent Space ‚Üí Decoder ‚Üí Output
#
# The goal is to minimize the difference between the input and the output.
# This helps the model learn the most important features of the data.
#
# Applications:
# - Dimensionality reduction
# - Image denoising
# - Anomaly detection
# - Data compression
# ------------------------------------------------------------

import numpy as np
import tensorflow 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, InputLayer
from tensorflow.keras.datasets import mnist
from tensorflow.keras import regularizers
import matplotlib.pyplot as plt


(x_train, _),(x_test, _) = mnist.load_data()


x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0


x_train = x_train.reshape((len(x_train), -1))  # flatten 28x28 -> 784
x_test = x_test.reshape((len(x_test), -1))


x_test.shape

# AUTOENCODER in SEQUENTIAL form
# ----------------------------
autoencoder = Sequential([
    # ----- Encoder -----
    InputLayer(input_shape=(784,)),
    Dense(128, activation='relu', activity_regularizer=regularizers.l1(1e-5)),
    Dense(64, activation='relu'),
    
    # ----- Latent representation -----
    Dense(32, activation='sigmoid'),
    
    # ----- Decoder -----
    Dense(64, activation='relu'),
    Dense(128, activation='relu'),
    Dense(784, activation='sigmoid')
])


# Compile model
autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


# Train
history = autoencoder.fit(
    x_train, x_train,
    epochs=70,
    batch_size=256,
    shuffle=True,
    validation_data=(x_test, x_test)
)

decoded_imgs = autoencoder.predict(x_test[:10])


index = 1  # choose any index between 0‚Äì9999 (for MNIST test set)

plt.figure(figsize=(6, 3))

# Original image
plt.subplot(1, 2, 1)
plt.imshow(x_test[index].reshape(28, 28), cmap="gray")
plt.title("Original")
plt.axis("off")

# Reconstructed image
plt.subplot(1, 2, 2)
plt.imshow(decoded_imgs[index].reshape(28, 28), cmap="gray")
plt.title("Reconstructed")
plt.axis("off")

plt.show()


-------------------------------------############-------------------------

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras import regularizers
(x_train, _), (x_test, _) = mnist.load_data()
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0
x_train = x_train.reshape((len(x_train), -1))
x_test = x_test.reshape((len(x_test), -1))
input_img = Input(shape=(784,))
encoded = Dense(128, activation="relu",activity_regularizer=regularizers.l1(1e-5))(input_img)  # sparsity
encoded = Dense(64, activation="relu")(encoded)
latent = Dense(32, activation="sigmoid")(encoded)  
decoded = Dense(64, activation="relu")(latent)
decoded = Dense(128, activation="relu")(decoded)
output_img = Dense(784, activation="sigmoid")(decoded)
autoencoder = Model(input_img, output_img)
autoencoder.compile(optimizer="adam", loss="binary_crossentropy")
history = autoencoder.fit(
    x_train, x_train,
    epochs=20,
    batch_size=256,
    shuffle=True,
    validation_data=(x_test, x_test)
)
decoded_imgs = autoencoder.predict(x_test[:10])
n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap="gray")
    plt.axis("off")
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap="gray")
    plt.axis("off")
plt.show()

   
-------------------------------------############-------------------------


# THEORY SUMMARY :- 

# Autoencoder = Encoder + Decoder

# Encoder:
   Compresses input (784-dimension) ‚Üí smaller latent space (32-dimension)
   Learns only the most important features.

# Decoder:
   Reconstructs the image from the latent space back to original form.

# Formulae:
   h = f(W1 * x + b1)        # Encoder output (hidden layer)
   x' = g(W2 * h + b2)       # Decoder reconstruction
   Loss = ||x - x'||¬≤  or binary_crossentropy(x, x')

# If the network reconstructs inputs well ‚Üí it has learned key features.

# Simple words:
 Autoencoders learn to ‚Äúsee‚Äù what matters most in data and recreate it
 from a compressed form ‚Äî like zipping and unzipping an image file.


